# Moral-AI
 
### Scenario 1:
Collaborative agents dont help others, but also dont lie. They are basically mute. They listen to every message that is sent to them about the location of a target by going to that exact location to check whether there is a target that belongs to them. Competitive agents lie to others in order to win the game. They lie about the location of targets. They lie about having collected more targets than they actually have. This leads to other competitive agents to give up if they are too far behind. Competitive agents also dont listen to anyones messages about the location of targets. Agents lose encouragement for every step that they take. If an agent collects a target, they get fuel back. If a competitive agent hears that an agent has collected 4 targets (lied to by other competitive agents) they lose encouragement. When they are out of encouragement, they give up and stop moving. All messages are boardcasted to all agents.
<br>
<br>

### Scenario 2:
Collaborative agents always try to help everyone else by telling them the location of targets and they send the message to the specific agent that the target belongs to. Collaborative agents listen to all the messages about target location and go to that location. Competitve agents do not listen to any messages. There is still encouragement lost from every step and gained from every target collected.
<br>
<br>

### Scenario 3:
Collaborative agents always try to help everyone else by telling them the location of targets and they send the message to the specific agent that the target belongs to. Collaborative agents listen to all the messages about target location and go to that location. Competitive agents lie about the target location to other agents, but dont listen to anyone else's messages. There is still encouragement lost from every step and gained from every target collected.
